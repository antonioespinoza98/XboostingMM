% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boost_mem.R
\name{boost_mem}
\alias{boost_mem}
\title{Iteration function for Tree-based MM}
\usage{
boost_mem(
  formula,
  data = NULL,
  random = NULL,
  shrinkage = 0.3,
  loss = "reg:squarederror",
  interaction.depth = 20,
  n.trees = 100,
  minsplit = 20,
  subsample = 0.5,
  lambda = 1,
  alpha = 0,
  conv_memboost = 0.001,
  maxIter_memboost = 100,
  minIter_memboost = 0,
  verbose_memboost = FALSE
)
}
\arguments{
\item{formula}{an object of class formula}

\item{data}{list or environment (or object coercible by `as.data.frame` to a data frame) containing the variables in the model.}

\item{random}{an object of class formula with the random intercept}

\item{shrinkage}{learning rate. Default: 0.1}

\item{loss}{by default it uses "reg:squarederror" more functions available in the `xgboost` documentation. Users, can pass a self-defined function to it.}

\item{interaction.depth}{maximum depth of the trees. Default: 1}

\item{n.trees}{number of trees to be generated. Default: 100}

\item{minsplit}{minimum observations for a tree to split. Default: 20}

\item{subsample}{sub-sample size for the tree training. Default: 0.5}

\item{lambda}{regularization term on weights. Default: 1}

\item{alpha}{regularization term on weights. Default: 0}

\item{conv_memboost}{Convergence threshold. Default: 0.001}

\item{maxIter_memboost}{maximum iterations. Default: 100}

\item{minIter_memboost}{minimum iterations. Default: 0}

\item{verbose_memboost}{Print information}
}
\value{
list of values
}
\description{
Function that leverages `xboosting()` to estimate the trees.
`predict.xgb()` for prediction, and `mem_boost_gll()` for convergence.
}
\references{
Marie Salditt, Sarah Humberg & Steffen Nestler (2023) Gradient Tree Boosting for Hierarchical Data, Multivariate Behavioral Research, 58:5, 911-937, DOI: 10.1080/00273171.2022.2146638
}
