% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xboosting.R
\name{xboosting}
\alias{xboosting}
\title{Computes Extreme Gradient Boosting Trees}
\usage{
xboosting(
  formula,
  data = NULL,
  loss = "reg:squarederror",
  n.trees = 100,
  shrinkage = 0.1,
  interaction.depth = 1,
  minsplit = 20,
  subsample = 0.5
)
}
\arguments{
\item{formula}{an object of class formula}

\item{data}{list or environment (or object coercible by `as.data.frame` to a data frame) containing the variables in the model.}

\item{loss}{by default it uses "reg:squarederror" more functions available in the `xgboost` documentation. Users, can pass a self-defined function to it.}

\item{n.trees}{number of trees to be generated. Default: 100}

\item{shrinkage}{learning rate. Default: 0.1}

\item{interaction.depth}{maximum depth of the trees. Default: 1}

\item{minsplit}{minimum observations for a tree to split. Default: 20}

\item{subsample}{sub-sample size for the tree training: Default: 0.5}
}
\value{
an xtremeBoost object
}
\description{
`xboosting()` relies on the Extreme Gradient Boosting model
to generate the decision trees and subsequently an assembly of m trees.
}
\references{
Chen T, He T, Benesty M, Khotilovich V, Tang Y, Cho H, Chen K, Mitchell R, Cano I,Zhou T, Li M, Xie J, Lin M, Geng Y, Li Y, Yuan J (2024). _xgboost: Extreme Gradient Boosting_. R package version 1.7.7.1,<https://CRAN.R-project.org/package=xgboost>.
Marie Salditt, Sarah Humberg & Steffen Nestler (2023) Gradient Tree Boosting for Hierarchical Data, Multivariate Behavioral Research, 58:5, 911-937, DOI: 10.1080/00273171.2022.2146638
}
